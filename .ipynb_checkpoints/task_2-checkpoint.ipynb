{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas II IF4044 Spark Job dengan Pyspark\n",
    "Muhammad Gilang Ramadhan 13520137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pyspark.sql.functions as psfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading json file for youtube video, Please Wait...\n",
      "Done.\n",
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2021-11-01|    1|\n",
      "|2021-10-25|    1|\n",
      "|2021-10-22|    1|\n",
      "|2021-10-20|    1|\n",
      "|2021-10-18|    1|\n",
      "|2021-10-16|    1|\n",
      "|2021-10-11|    2|\n",
      "|2021-10-09|    1|\n",
      "|2021-10-04|    1|\n",
      "|2021-10-03|    1|\n",
      "|2021-10-01|    1|\n",
      "|2021-07-30|    1|\n",
      "|2021-07-26|    1|\n",
      "|2021-07-24|    1|\n",
      "|2021-07-23|    1|\n",
      "|2021-07-20|    1|\n",
      "|2021-07-19|    1|\n",
      "|2021-07-17|    1|\n",
      "|2021-07-12|    1|\n",
      "|2021-07-05|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading json file for youtube video, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:9000/dummy/youtube_video_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct data frame\n",
    "dist_dataframe = dataframe.coalesce(1).select(\"id\",psfunc.to_date(\"snippet.publishedAt\").alias(\"date\")).distinct()\n",
    "youtube_video_df = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))\n",
    "youtube_video_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading json file for youtube comment, Please Wait...\n",
      "Done.\n",
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2021-10-08|  105|\n",
      "|2021-04-13|   45|\n",
      "|2021-04-14|    7|\n",
      "|2021-04-20|  126|\n",
      "|2021-02-23|    1|\n",
      "|2021-02-26|    2|\n",
      "|2021-02-25|    6|\n",
      "|2021-02-24|   47|\n",
      "|2021-10-09|   31|\n",
      "|2021-12-07|   78|\n",
      "|2021-07-15|    1|\n",
      "|2021-06-30|    3|\n",
      "|2021-06-26|    1|\n",
      "|2021-06-20|    2|\n",
      "|2021-06-17|    2|\n",
      "|2021-06-15|    2|\n",
      "|2021-06-14|    2|\n",
      "|2021-06-10|    2|\n",
      "|2021-06-08|    2|\n",
      "|2021-06-07|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading json file for youtube comment, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:9000/dummy/youtube_comment_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(\"snippet.topLevelComment.id\",psfunc.to_date(\"snippet.topLevelComment.snippet.publishedAt\").alias(\"date\")).distinct()\n",
    "youtube_comment_df = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))\n",
    "youtube_comment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|social_media|      date|count|\n",
      "+------------+----------+-----+\n",
      "|     youtube|2021-11-01|    1|\n",
      "|     youtube|2021-10-25|    1|\n",
      "|     youtube|2021-10-22|    1|\n",
      "|     youtube|2021-10-20|    1|\n",
      "|     youtube|2021-10-18|    1|\n",
      "|     youtube|2021-10-16|    1|\n",
      "|     youtube|2021-10-11|    2|\n",
      "|     youtube|2021-10-09|    1|\n",
      "|     youtube|2021-10-04|    1|\n",
      "|     youtube|2021-10-03|    1|\n",
      "|     youtube|2021-10-01|    1|\n",
      "|     youtube|2021-07-30|    1|\n",
      "|     youtube|2021-07-26|    1|\n",
      "|     youtube|2021-07-24|    1|\n",
      "|     youtube|2021-07-23|    1|\n",
      "|     youtube|2021-07-20|    1|\n",
      "|     youtube|2021-07-19|    1|\n",
      "|     youtube|2021-07-17|    1|\n",
      "|     youtube|2021-07-12|    1|\n",
      "|     youtube|2021-07-05|    1|\n",
      "+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge youtube video and youtube comment dataframe to mergeAll variable\n",
    "df_merge = youtube_video_df.union(youtube_comment_df)\n",
    "youtube_merge_df_dist = df_merge.coalesce(1)\n",
    "mergeAll_df = youtube_merge_df_dist.select(psfunc.lit(\"youtube\").alias(\"social_media\"), psfunc.col(\"date\"), psfunc.col(\"count\"))\n",
    "mergeAll_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instagram Comment, Post, dan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading json file for instagram comment,Post,and Status, Please Wait...\n",
      "Done.\n",
      "+-------------------+----------+\n",
      "|                 id|      date|\n",
      "+-------------------+----------+\n",
      "|2771817423832028073|2022-02-12|\n",
      "|2771816395363456296|2022-02-12|\n",
      "|2771813732960431478|2022-02-12|\n",
      "|2771812251238654358|2022-02-12|\n",
      "|2771812368996891591|2022-02-12|\n",
      "|2771812115260805336|2022-02-12|\n",
      "|2771812023208929543|2022-02-12|\n",
      "|2771807155436487440|2022-02-12|\n",
      "|2771806109503860403|2022-02-12|\n",
      "|2771805198794716662|2022-02-12|\n",
      "|2771804737846527508|2022-02-12|\n",
      "|2771799811530076247|2022-02-12|\n",
      "|2771799647482351483|2022-02-12|\n",
      "|2771795452331165096|2022-02-12|\n",
      "|2771795262773872288|2022-02-12|\n",
      "|2771795040920354771|2022-02-12|\n",
      "|2771794627225093819|2022-02-12|\n",
      "|2771794454120283530|2022-02-12|\n",
      "|2771793697237403132|2022-02-12|\n",
      "|2771791031607752475|2022-02-12|\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading json file for instagram comment,Post,and Status, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:9000/dummy/instagram_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"), psfunc.from_unixtime(timestamp=\"created_time\", format=\"yyyy-MM-dd\").alias(\"date\"))\n",
    "instagram_comment_dataframe = dist_dataframe\n",
    "instagram_comment_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|social_media|      date|count|\n",
      "+------------+----------+-----+\n",
      "|   instagram|2022-02-12|  459|\n",
      "|   instagram|2022-02-11| 1064|\n",
      "|   instagram|2022-02-10| 1031|\n",
      "|   instagram|2022-02-09| 1249|\n",
      "|   instagram|2022-02-07| 1069|\n",
      "|   instagram|2022-01-15|  232|\n",
      "|   instagram|2022-02-08|  918|\n",
      "|   instagram|2022-01-18|  354|\n",
      "|   instagram|2022-01-17|  425|\n",
      "|   instagram|2022-01-12|  140|\n",
      "|   instagram|2022-02-06|  400|\n",
      "|   instagram|2022-02-05|  724|\n",
      "|   instagram|2022-02-03|  951|\n",
      "|   instagram|2022-02-04| 1311|\n",
      "|   instagram|2022-02-02|  660|\n",
      "|   instagram|2022-01-27|  380|\n",
      "|   instagram|2022-01-31|  492|\n",
      "|   instagram|2022-02-01|  791|\n",
      "|   instagram|2022-02-17|   98|\n",
      "|   instagram|2022-02-16|  203|\n",
      "+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge All instagram contents with mergeAll_df variable\n",
    "distinct_instagram_dataframe = instagram_comment_dataframe.coalesce(1).distinct().groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))\n",
    "temp_merge_df = distinct_instagram_dataframe.select(psfunc.lit(\"instagram\").alias(\"social_media\"), psfunc.col(\"date\"), psfunc.col(\"count\")).union(mergeAll_df)\n",
    "mergeAll_df = temp_merge_df.coalesce(1)\n",
    "mergeAll_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading json file for facebook post, Please Wait...\n",
      "Done.\n",
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2022-01-01|    2|\n",
      "|2021-12-31|    1|\n",
      "|2021-12-29|    1|\n",
      "|2021-12-28|    3|\n",
      "|2021-12-27|    5|\n",
      "|2021-12-25|    1|\n",
      "|2021-12-24|    1|\n",
      "|2021-12-23|    1|\n",
      "|2021-12-22|    3|\n",
      "|2021-12-21|    3|\n",
      "|2021-12-20|    1|\n",
      "|2021-12-18|    1|\n",
      "|2021-12-17|    7|\n",
      "|2021-12-16|    1|\n",
      "|2021-12-15|    3|\n",
      "|2021-12-14|    6|\n",
      "|2021-12-11|    1|\n",
      "|2021-12-10|    2|\n",
      "|2021-12-09|    3|\n",
      "|2021-12-08|    8|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading json file for facebook post, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:9000/dummy/facebook_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"), psfunc.to_date(\"created_time\").alias(\"date\")).distinct()\n",
    "facebook_post_dataframe = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))\n",
    "facebook_post_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|social_media|      date|count|\n",
      "+------------+----------+-----+\n",
      "|    facebook|2022-01-01|    2|\n",
      "|    facebook|2021-12-31|    1|\n",
      "|    facebook|2021-12-29|    1|\n",
      "|    facebook|2021-12-28|    3|\n",
      "|    facebook|2021-12-27|    5|\n",
      "|    facebook|2021-12-25|    1|\n",
      "|    facebook|2021-12-24|    1|\n",
      "|    facebook|2021-12-23|    1|\n",
      "|    facebook|2021-12-22|    3|\n",
      "|    facebook|2021-12-21|    3|\n",
      "|    facebook|2021-12-20|    1|\n",
      "|    facebook|2021-12-18|    1|\n",
      "|    facebook|2021-12-17|    7|\n",
      "|    facebook|2021-12-16|    1|\n",
      "|    facebook|2021-12-15|    3|\n",
      "|    facebook|2021-12-14|    6|\n",
      "|    facebook|2021-12-11|    1|\n",
      "|    facebook|2021-12-10|    2|\n",
      "|    facebook|2021-12-09|    3|\n",
      "|    facebook|2021-12-08|    8|\n",
      "+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge facebook post with mergeAll_df variable\n",
    "temp_merge_df = facebook_post_dataframe.select(psfunc.lit(\"facebook\").alias(\"social_media\"), psfunc.col(\"date\"), psfunc.col(\"count\")).union(mergeAll_df)\n",
    "mergeAll_df = temp_merge_df.coalesce(1)\n",
    "mergeAll_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading json file for twitter status, Please Wait...\n",
      "Done.\n",
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2022-01-05|  930|\n",
      "|2022-01-08|  206|\n",
      "|2022-01-12| 1599|\n",
      "|2022-01-04|  301|\n",
      "|2022-01-06|  676|\n",
      "|2022-01-02|   83|\n",
      "|2022-01-03|   45|\n",
      "|2022-01-01|   30|\n",
      "|2021-12-09|  892|\n",
      "|2021-12-08|  590|\n",
      "|2021-12-11| 1919|\n",
      "|2021-12-15|  471|\n",
      "|2021-12-14|  548|\n",
      "|2022-01-11|  818|\n",
      "|2021-12-22|  216|\n",
      "|2021-12-16| 1259|\n",
      "|2021-12-12|  610|\n",
      "|2021-12-10| 1661|\n",
      "|2021-12-13|  306|\n",
      "|2021-12-24|  333|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading json file for twitter status, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:9000/dummy/twitter_status_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"),psfunc.to_date(psfunc.substring(\"created_at\", 5, 26), \"MMM dd HH:mm:ss Z yyyy\").alias(\"date\")).distinct()\n",
    "twitter_status_dataframe = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))\n",
    "twitter_status_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|social_media|      date|count|\n",
      "+------------+----------+-----+\n",
      "|     twitter|2022-01-05|  930|\n",
      "|     twitter|2022-01-08|  206|\n",
      "|     twitter|2022-01-12| 1599|\n",
      "|     twitter|2022-01-04|  301|\n",
      "|     twitter|2022-01-06|  676|\n",
      "|     twitter|2022-01-02|   83|\n",
      "|     twitter|2022-01-03|   45|\n",
      "|     twitter|2022-01-01|   30|\n",
      "|     twitter|2021-12-09|  892|\n",
      "|     twitter|2021-12-08|  590|\n",
      "|     twitter|2021-12-11| 1919|\n",
      "|     twitter|2021-12-15|  471|\n",
      "|     twitter|2021-12-14|  548|\n",
      "|     twitter|2022-01-11|  818|\n",
      "|     twitter|2021-12-22|  216|\n",
      "|     twitter|2021-12-16| 1259|\n",
      "|     twitter|2021-12-12|  610|\n",
      "|     twitter|2021-12-10| 1661|\n",
      "|     twitter|2021-12-13|  306|\n",
      "|     twitter|2021-12-24|  333|\n",
      "+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge twitter status with mergeAll_df variable\n",
    "temp_merge_df = twitter_status_dataframe.select(psfunc.lit(\"twitter\").alias(\"social_media\"), psfunc.col(\"date\"), psfunc.col(\"count\")).union(mergeAll_df)\n",
    "mergeAll_df = temp_merge_df.coalesce(1)\n",
    "mergeAll_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing all merged dataframe to output csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing all merged files, please wait.....\n",
      "Files were writen, done.\n",
      "Duration: 28.779454469680786 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing all merged files, please wait.....\")\n",
    "timeCount = time.time()\n",
    "mergeAll_df.write.csv(f\"hdfs://localhost:9000/outputCsv_sparkjob\")\n",
    "timeCount = time.time() - timeCount\n",
    "print(\"Files were writen, done.\")\n",
    "print(\"Duration: {} s\". format(timeCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
