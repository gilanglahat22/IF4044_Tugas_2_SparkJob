{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas II IF4044 Spark Job dengan Pyspark\n",
    "Muhammad Gilang Ramadhan 13520137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pyspark.sql.functions as psfunc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for youtube video, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/youtube_video_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct data frame\n",
    "dist_dataframe = dataframe.coalesce(1).select(\"id\",psfunc.to_date(\"snippet.publishedAt\").alias(\"date\")).distinct()\n",
    "youtube_video_df = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for youtube comment, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/youtube_comment_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(\"snippet.topLevelComment.id\",psfunc.to_date(\"snippet.topLevelComment.snippet.publishedAt\").alias(\"date\")).distinct()\n",
    "youtube_comment_df = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge youtube video and youtube comment dataframe to mergeAll variable\n",
    "df_merge = youtube_video_df.union(youtube_comment_df)\n",
    "youtube_merge_df_dist = df_merge.coalesce(1)\n",
    "\n",
    "mergeAll_df = youtube_merge_df_dist.select(psfunc.col(\"date\"), psfunc.lit(\"youtube\").alias(\"socmed\"), psfunc.col(\"count\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instagram Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for instagram comment, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/instagram_comment_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"), psfunc.from_unixtime(timestamp=\"created_time\", format=\"yyyy-MM-dd\").alias(\"date\"))\n",
    "instagram_comment_dataframe = dist_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instagram Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for instagram post, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/instagram_post_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"), psfunc.from_unixtime(timestamp=\"created_time\", format=\"yyyy-MM-dd\").alias(\"date\"))\n",
    "instagram_post_dataframe = dist_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instagram Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for instagram status, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/instagram_status_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"), psfunc.from_unixtime(timestamp=\"created_time\", format=\"yyyy-MM-dd\").alias(\"date\"))\n",
    "instagram_status_dataframe = dist_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all instagram content dataframe and merge with mergeAll_df variable\n",
    "df_merge = instagram_comment_dataframe.union(instagram_post_dataframe)\n",
    "df_merge = instagram_status_dataframe.union(df_merge)\n",
    "instagram_merge_df_dist = df_merge.coalesce(1)\n",
    "\n",
    "temp_merge_df = instagram_merge_df_dist.select(psfunc.col(\"date\"), psfunc.lit(\"instagram\").alias(\"socmed\"), psfunc.col(\"count\")).union(mergeAll_df)\n",
    "mergeAll_df = temp_merge_df.coalesce(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for facebook post, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/facebook_post_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"), psfunc.to_date(\"created_time\").alias(\"date\")).distinct()\n",
    "facebook_post_dataframe = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge facebook post with mergeAll_df variable\n",
    "temp_merge_df = facebook_post_dataframe.select(psfunc.col(\"date\"), psfunc.lit(\"facebook\").alias(\"socmed\"), psfunc.col(\"count\")).union(mergeAll_df)\n",
    "mergeAll_df = temp_merge_df.coalesce(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading json file for facebook post, Please Wait...\")\n",
    "dataframe = spark.read.json(f\"hdfs://localhost:8888/raw_json/twitter_status_*.json\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# get distinct dataframe\n",
    "dist_dataframe = dataframe.coalesce(1).select(psfunc.col(\"id\"),psfunc.to_date(psfunc.substring(\"created_at\", 5, 26), \"MMM dd HH:mm:ss Z yyyy\").alias(\"date\")).distinct()\n",
    "twitter_status_dataframe = dist_dataframe.groupBy(\"date\").agg(psfunc.count(\"id\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge twitter status with mergeAll_df variable\n",
    "temp_merge_df = twitter_status_dataframe.select(psfunc.col(\"date\"), psfunc.lit(\"twitter\").alias(\"socmed\"), psfunc.col(\"count\")).union(mergeAll_df)\n",
    "mergeAll_df = temp_merge_df.coalesce(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing all merged files, please wait.....\")\n",
    "timeCount = time.time()\n",
    "mergeAll_df.write.csv(f\"hdfs://localhost:8888/output_csv_tugas2\")\n",
    "timeCount = time.time() - timeCount\n",
    "print(\"Files were writen, done.\")\n",
    "print(\"Duration: {} s\". format(timeCount))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
